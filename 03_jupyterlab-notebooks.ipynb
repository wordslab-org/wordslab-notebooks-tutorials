{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3ba61c-ce5c-44e5-bb05-c2ba016d08d3",
   "metadata": {},
   "source": [
    "# Tutorial 3 - JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc537f-e6ab-4100-9e8b-0049aebc8c56",
   "metadata": {},
   "source": [
    "## Discover the Jupyterlab development environment\n",
    "\n",
    "Main features\n",
    "- [The JupyterLab Interface](https://jupyterlab.readthedocs.io/en/latest/user/interface.html)\n",
    "- [Working with Files](https://jupyterlab.readthedocs.io/en/latest/user/files.html)\n",
    "- [Notebooks](https://jupyterlab.readthedocs.io/en/latest/user/notebook.html)\n",
    "- [Terminals](https://jupyterlab.readthedocs.io/en/latest/user/terminal.html)\n",
    "\n",
    "Left sidebar\n",
    "- [Managing Kernels and Terminals](https://jupyterlab.readthedocs.io/en/latest/user/running.html)\n",
    "- [CPU and GPU usage dahsboards](https://github.com/rapidsai/jupyterlab-nvdashboard#jupyterlab-nvdashboard)\n",
    "- [Git version control](https://github.com/jupyterlab/jupyterlab-git#jupyterlab-git)\n",
    "\n",
    "Development tools\n",
    "- [Debugger](https://jupyterlab.readthedocs.io/en/latest/user/debugger.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb3a91-eb84-49c0-bf9a-77ac1d9b3108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:04:21.776862Z",
     "iopub.status.busy": "2025-10-05T12:04:21.776701Z",
     "iopub.status.idle": "2025-10-05T12:04:21.779094Z",
     "shell.execute_reply": "2025-10-05T12:04:21.778730Z",
     "shell.execute_reply.started": "2025-10-05T12:04:21.776851Z"
    }
   },
   "source": [
    "## Jupyter AI\n",
    "\n",
    "https://jupyter-ai.readthedocs.io/en/latest/users/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb04ce4-1330-47ee-8966-97b605034ad6",
   "metadata": {},
   "source": [
    "### What is Jupyter AI ?\n",
    "\n",
    "Jupyter AI is an innovative tool under incubation as part of the JupyterLab organization. \n",
    "\n",
    "It integrates generative AI with Jupyter notebooks, providing a user-friendly and powerful way to explore generative AI models. \n",
    "\n",
    "The main features of Jupyter AI include:\n",
    "\n",
    "1. **%%ai Magic Command**: This feature turns the Jupyter notebook into a reproducible generative AI playground. It works across various platforms where the IPython kernel runs, including JupyterLab, Jupyter Notebook, Google Colab, Kaggle, and VSCode.\n",
    "\n",
    "2. **Native Chat UI in JupyterLab**: Jupyter AI offers a native chat user interface within JupyterLab, allowing users to interact with generative AI as a conversational assistant.\n",
    "\n",
    "3. **Support for Multiple Generative Model Providers**: Jupyter AI supports a wide range of generative model providers, including AI21, Anthropic, AWS, Cohere, Gemini, Hugging Face, MistralAI, NVIDIA, and OpenAI.\n",
    "\n",
    "4. **Local Model Support**: It provides support for local models through GPT4All and Ollama, enabling the use of generative AI models on consumer-grade machines with ease and privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8eb5b1-a9d9-4dd7-9866-b05591e51100",
   "metadata": {},
   "source": [
    "### How to configure Jupyter AI for the first use ?\n",
    "\n",
    "To verify that Jupyter AI is properly installed in JupyterLab, follow these steps:\n",
    "\n",
    "#### Check if the Jupyter AI Extension is installed\n",
    "\n",
    "Once JupyterLab is open, look for a new tool icon labeled \"Jupyter AI Chat\" in the left-hand sidebar.\n",
    "\n",
    "If you see this icon, it indicates that the Jupyter AI extension has been successfully installed and integrated into JupyterLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e5252-2127-4b8c-8bbf-cfd3582ae0b1",
   "metadata": {},
   "source": [
    "#### Configure connections to language models\n",
    "\n",
    "You need to configure a language model and an embedding model in Jupyter AI:\n",
    "\n",
    "**Language Model Configuration**\n",
    "\n",
    "- A language model responds to users' messages in the chat panel by accepting a prompt and producing a response.\n",
    "- Language models are typically pre-trained and ready to use, but it's important to be aware of their biases and incomplete training sets.\n",
    "\n",
    "**Embedding Model Configuration**:\n",
    "\n",
    "- An embedding model is used for learning and asking about local data. These models can transform your data, including documents and source code files, into vectors that help Jupyter AI compose prompts to language models.\n",
    "- Your language model and your embedding model do not need to be provided by the same vendor, but you will need authentication credentials for each model provider that you use.\n",
    "\n",
    "For both models, ensure you have the necessary authentication credentials from the respective providers. This setup allows Jupyter AI to effectively utilize both types of models for various tasks.\n",
    "\n",
    "To select and configure models in Jupyter AI, follow the following steps.\n",
    "\n",
    "**Step 1 - Configure the models**\n",
    "\n",
    "Open the Chat Interface\n",
    "\n",
    "- Once you have started JupyterLab, click the new \"chat\" icon in the left side panel to open the chat interface. \n",
    "\n",
    "Select Models\n",
    "\n",
    "- The first time you open the chat interface, Jupyter AI will prompt you to select which models you want to use as a language model and as an embedding model.\n",
    "- Users may select a language model and, optionally, an embedding model. You should select one of each so that you can use the full functionality of the chat interface.\n",
    "- After making your selections, the UI may display text boxes for one or more settings keys.\n",
    "\n",
    "In the **Completion model** dropdown list, select:\n",
    "\n",
    "```\n",
    "Ollama::*\n",
    "```\n",
    "\n",
    "In the local model ID text input field, type:\n",
    "\n",
    "```\n",
    "llama3.1:8b\n",
    "```\n",
    "\n",
    "In the **Embedding model** dropdown list, select:\n",
    "\n",
    "```\n",
    "Ollama::nomic-embed-text\n",
    "```\n",
    "\n",
    "Click on the gear icon on the right of the title Inline completions model:\n",
    "\n",
    "- Click on the second checkbox: Enabled - Whether to fetch completions **JupyterAI provider**.\n",
    "\n",
    "After you enabled the JupyterAI provider: the dropdown list below **Inline completions model** is activated.\n",
    "\n",
    "Select:\n",
    "\n",
    "```\n",
    "Ollama::*\n",
    "```\n",
    "\n",
    "In the local model ID text input field just below, type:\n",
    "\n",
    "```\n",
    "qwen2.5-coder:1.5b-base\n",
    "```\n",
    "\n",
    "When everything is ready:\n",
    "\n",
    "- Click on Save Changes button at the bottom.\n",
    "- Then Click on the left pointing arrow at the top.\n",
    "\n",
    "**Step 2 - Test chat**\n",
    "\n",
    "Enter \"hello\" in the Ask Jupyternaut textbox then press Enter.\n",
    "\n",
    "Make sure you get a reponse from the AI assistant (it could take a few seconds to load it in memory).\n",
    "\n",
    "After typing a prompt, be aware that you can click on the little arrow oriented towards the bottom at the right of the input text box to **include the currently selected cells in the context** of the question .\n",
    "\n",
    "You can also **include a complete file** from the workspace (the **base path is /home/workspace**):\n",
    "\n",
    "```\n",
    "summarize @file:wordslab-notebooks-tutorials/06_learn_jupyter_ai.ipynb\n",
    "```\n",
    "\n",
    "**Step 3 - Test embeddings**\n",
    "\n",
    "Type \"/learn /home/workspace/wordslab-notebooks-tutorials/06_learn_jupyter_ai.ipynb\" in the Ask Jupyternaut textbox then press Enter.\n",
    "\n",
    "Make sure the response looks like :\n",
    "\n",
    "```\n",
    "ðŸŽ‰ I have learned documents at /home/workspace/wordslab-notebooks-tutorials/01_explore_hardware.ipynb and I am ready to answer questions about them. You can ask questions about these docs by prefixing your message with /ask.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde1f05f-4aab-4bb7-8661-3c9ddb0d3890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:12:17.798968Z",
     "iopub.status.busy": "2025-10-05T12:12:17.798511Z",
     "iopub.status.idle": "2025-10-05T12:12:17.804604Z",
     "shell.execute_reply": "2025-10-05T12:12:17.803592Z",
     "shell.execute_reply.started": "2025-10-05T12:12:17.798940Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f09a92-f7fb-4cfb-acf3-f3eaf1da5b6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:06:22.791027Z",
     "iopub.status.busy": "2025-10-05T12:06:22.790516Z",
     "iopub.status.idle": "2025-10-05T12:06:22.796173Z",
     "shell.execute_reply": "2025-10-05T12:06:22.795651Z",
     "shell.execute_reply.started": "2025-10-05T12:06:22.791005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma3:27b', 'qwen3-coder:30b')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = os.getenv(\"OLLAMA_CHAT_MODEL\")\n",
    "code_model = os.getenv(\"OLLAMA_CODE_MODEL\")\n",
    "chat_model,code_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77f89b-ab1d-44cb-99d7-67ebfc20b2f8",
   "metadata": {},
   "source": [
    "#### Load the IPython extension in your notebook\n",
    "\n",
    "Before you can use the `%%ai` magic command in your Jupyter notebook, you need to load the IPython extension by running the following code in a notebook cell or IPython shell:\n",
    "\n",
    "```\n",
    "%load_ext jupyter_ai_magics\n",
    "```\n",
    "\n",
    "This command loads the necessary extension that enables the use of AI magics, including `%%ai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d8a71-5d38-4191-81de-a70f771c2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac94eca-910b-453c-845c-b7f83908f188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:06:30.914133Z",
     "iopub.status.busy": "2025-10-05T12:06:30.913737Z",
     "iopub.status.idle": "2025-10-05T12:06:30.920936Z",
     "shell.execute_reply": "2025-10-05T12:06:30.920039Z",
     "shell.execute_reply.started": "2025-10-05T12:06:30.914108Z"
    }
   },
   "outputs": [],
   "source": [
    "%config AiMagics.default_language_model = \"ollama:{chat_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0eebc3-7268-4949-9578-36b2a01a51d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:06:35.573544Z",
     "iopub.status.busy": "2025-10-05T12:06:35.573378Z",
     "iopub.status.idle": "2025-10-05T12:06:35.577109Z",
     "shell.execute_reply": "2025-10-05T12:06:35.576676Z",
     "shell.execute_reply.started": "2025-10-05T12:06:35.573534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered new alias `chat`"
      ],
      "text/plain": [
       "Registered new alias `chat`"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai register chat ollama:{chat_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc08c9c5-2f5c-4ef5-94c4-467546e141dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:06:40.211250Z",
     "iopub.status.busy": "2025-10-05T12:06:40.211087Z",
     "iopub.status.idle": "2025-10-05T12:06:40.215002Z",
     "shell.execute_reply": "2025-10-05T12:06:40.214609Z",
     "shell.execute_reply.started": "2025-10-05T12:06:40.211241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Registered new alias `code`"
      ],
      "text/plain": [
       "Registered new alias `code`"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai register code ollama:{code_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77447f41-cd7c-42a8-a6d7-5b259fa82d9e",
   "metadata": {},
   "source": [
    "### Try to use the `%%ai` Magic Command\n",
    "\n",
    "Once the extension is loaded, you can run %%ai cell magic commands and %ai line magic commands. \n",
    "\n",
    "For example, you can get help with syntax by running %%ai help or %ai help. You can also pass --help as an argument to any line magic command (e.g., %ai list --help) to learn about what the command does and how to use it\n",
    "\n",
    "In a code cell within the notebook, type:\n",
    " ```python\n",
    " %ai help\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aeff342-7b81-4aec-bd90-27c116e9965e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:14:24.524369Z",
     "iopub.status.busy": "2025-10-05T12:14:24.523510Z",
     "iopub.status.idle": "2025-10-05T12:14:24.566187Z",
     "shell.execute_reply": "2025-10-05T12:14:24.565741Z",
     "shell.execute_reply.started": "2025-10-05T12:14:24.524250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] [MODEL_ID]\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  reset     Clear the conversation transcript.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "  version   Prints Jupyter-AI version\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%ai help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd9d4a-56b2-4e04-bc56-f9a239550a70",
   "metadata": {},
   "source": [
    "If the `%ai` magic command is recognized and you can run AI-related commands without errors, it confirms that Jupyter AI is properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3fc7c8c-498c-47c3-840b-994399077642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:13:35.326409Z",
     "iopub.status.busy": "2025-10-05T12:13:35.325987Z",
     "iopub.status.idle": "2025-10-05T12:14:12.411910Z",
     "shell.execute_reply": "2025-10-05T12:14:12.409128Z",
     "shell.execute_reply.started": "2025-10-05T12:13:35.326381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Jupyter AI is a JupyterLab extension that brings the power of Large Language Models (LLMs) directly into your notebook environment. It acts as an AI assistant, helping with tasks like code generation, explanation, documentation, and even fixing errorsâ€”all within your notebook. It integrates seamlessly with popular LLMs like OpenAI's GPT models and local alternatives, allowing you to use AI to boost your coding productivity and understanding through simple, inline commands within Markdown cells. It's a powerful tool for data scientists, developers, and anyone looking to leverage AI in their Jupyter workflows."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gemma3:27b",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chat\n",
    "What is jupyter-ai ? Describe the tool in a short paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622203e8-b108-46b8-b629-1be3947e246f",
   "metadata": {},
   "source": [
    " You can select the format used to render the output with the flag :\n",
    "\n",
    " ```\n",
    " -f code | html | image | json | markdown | math | md | text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f0121e-3582-43e7-8fb0-dd89eb42932f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:07:11.807114Z",
     "iopub.status.busy": "2025-10-05T12:07:11.806998Z",
     "iopub.status.idle": "2025-10-05T12:07:28.915343Z",
     "shell.execute_reply": "2025-10-05T12:07:28.914648Z",
     "shell.execute_reply.started": "2025-10-05T12:07:11.807105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "qwen3-coder:30b",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai code -f code\n",
    "Generate a program to compute pi with low precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93cdc091-43d4-4a3d-a667-ee62f657859b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:07:43.133014Z",
     "iopub.status.busy": "2025-10-05T12:07:43.132831Z",
     "iopub.status.idle": "2025-10-05T12:07:43.137634Z",
     "shell.execute_reply": "2025-10-05T12:07:43.137266Z",
     "shell.execute_reply.started": "2025-10-05T12:07:43.133003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi (low precision): 3.16\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_pi_low_precision():\n",
    "    # Using the Monte Carlo method with low precision\n",
    "    # Generate random points and count how many fall inside the unit circle\n",
    "    import random\n",
    "    \n",
    "    # Use a small number of iterations for low precision\n",
    "    iterations = 10000\n",
    "    inside_circle = 0\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "        if x*x + y*y <= 1:\n",
    "            inside_circle += 1\n",
    "    \n",
    "    # Estimate pi using the ratio of points inside the circle to total points\n",
    "    pi_estimate = 4 * inside_circle / iterations\n",
    "    return round(pi_estimate, 2)\n",
    "\n",
    "# Compute and print the result\n",
    "result = compute_pi_low_precision()\n",
    "print(f\"Pi (low precision): {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5903d887-15bf-4c36-a5e4-ff6590d841f1",
   "metadata": {},
   "source": [
    "By default, two previous Human/AI message exchanges are included in the context of the new prompt.\n",
    "\n",
    "You can change this using the IPython %config magic, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "096f390d-3d9a-4c41-98c4-7147ccc9b612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:15:38.708484Z",
     "iopub.status.busy": "2025-10-05T12:15:38.708097Z",
     "iopub.status.idle": "2025-10-05T12:15:38.743379Z",
     "shell.execute_reply": "2025-10-05T12:15:38.742927Z",
     "shell.execute_reply.started": "2025-10-05T12:15:38.708458Z"
    }
   },
   "outputs": [],
   "source": [
    "%config AiMagics.max_history = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc51a57-dfb9-4076-a6ec-e08d856c6390",
   "metadata": {},
   "source": [
    "You can reference Python variables in prompt, including In, Out or Error code cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51570b5b-28e2-4ae9-a70d-97148a9c2d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:16:46.098877Z",
     "iopub.status.busy": "2025-10-05T12:16:46.098631Z",
     "iopub.status.idle": "2025-10-05T12:17:07.053542Z",
     "shell.execute_reply": "2025-10-05T12:17:07.053132Z",
     "shell.execute_reply.started": "2025-10-05T12:16:46.098867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Explanation of the Python Code\n",
       "\n",
       "This Python code estimates the value of Pi using the Monte Carlo method. Here's a breakdown:\n",
       "\n",
       "**1. Imports:**\n",
       "\n",
       "*   `import math`: Although not directly used in this simplified version, it's generally good practice to import the `math` module when dealing with mathematical calculations.\n",
       "*   `import random`: This module is essential for generating random numbers, which are the core of the Monte Carlo simulation.\n",
       "\n",
       "**2. `compute_pi_low_precision()` Function:**\n",
       "\n",
       "*   **Monte Carlo Method:**  This function uses the Monte Carlo method, a technique that relies on random sampling to obtain numerical results. In this case, it's used to approximate Pi.\n",
       "*   **`iterations = 10000`:**  This line sets the number of random points that will be generated. A smaller number of iterations results in lower precision but faster computation.\n",
       "*   **`inside_circle = 0`:** This variable is a counter that keeps track of how many random points fall *inside* a unit circle (a circle with radius 1 centered at the origin).\n",
       "*   **`for _ in range(iterations):`:** This loop iterates `iterations` (10000) times, generating a new random point in each iteration.\n",
       "*   **`x = random.uniform(-1, 1)` and `y = random.uniform(-1, 1)`:**  These lines generate random floating-point numbers between -1 and 1 for the x and y coordinates of a point. This effectively creates points within a square with sides of length 2 centered at the origin.\n",
       "*   **`if x*x + y*y <= 1:`:** This is the core of the Monte Carlo estimation.  It checks if the randomly generated point (x, y) lies inside the unit circle. The equation `xÂ² + yÂ² <= 1` represents the condition for a point to be inside the unit circle.\n",
       "*   **`inside_circle += 1`:** If the point (x, y) is inside the unit circle, the `inside_circle` counter is incremented.\n",
       "*   **`pi_estimate = 4 * inside_circle / iterations`:** After generating all the random points, the value of Pi is estimated using this formula. This formula is derived from the ratio of the area of the circle to the area of the enclosing square.\n",
       "*   **`return round(pi_estimate, 2)`:** The estimated value of Pi is rounded to two decimal places using the `round()` function, and then returned.\n",
       "\n",
       "**3. Main Execution Block:**\n",
       "\n",
       "*   **`result = compute_pi_low_precision()`:**  This line calls the `compute_pi_low_precision()` function and stores the returned estimated value of Pi in the `result` variable.\n",
       "*   **`print(f\"Pi (low precision): 3.16\")`:** This line prints the estimated value of Pi to the console. **Important Note:** The code *always* prints `3.16` regardless of the value computed by the function. This is likely an oversight or a deliberate simplification for demonstration purposes. It doesn't use the `result` variable which stores the computed pi estimate.\n",
       "\n",
       "**In essence, the code simulates throwing darts at a square board with a circle inscribed within it.  The ratio of darts that land inside the circle to the total number of darts thrown approximates the ratio of the circle's area to the square's area, allowing us to estimate Pi.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gemma3:27b",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai\n",
    "Please explain the code below:\n",
    "--\n",
    "{In[8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b776f-5ec2-4536-94df-6098a64bcb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-notebooks-tutorials",
   "language": "python",
   "name": "wordslab-notebooks-tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
